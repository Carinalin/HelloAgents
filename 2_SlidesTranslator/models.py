import os
import streamlit as st
from dotenv import load_dotenv, find_dotenv  # æ–°å¢ï¼šå¯¼å…¥dotenvç›¸å…³å‡½æ•°
from langchain.chat_models import init_chat_model
from langchain_core.exceptions import LangChainException

# ====================== 1. åŠ è½½.envæ–‡ä»¶ ======================
def load_env_file():
    """
    åŠ è½½å½“å‰ç›®å½•ï¼ˆåŠä¸Šçº§ç›®å½•ï¼‰çš„.envæ–‡ä»¶ï¼Œå°†å…¶ä¸­çš„å˜é‡æ³¨å…¥ç³»ç»Ÿç¯å¢ƒå˜é‡
    ä¼˜å…ˆçº§ï¼šç³»ç»Ÿç¯å¢ƒå˜é‡ > .envæ–‡ä»¶å˜é‡ï¼ˆload_dotenvé»˜è®¤ä¸è¦†ç›–å·²å­˜åœ¨çš„ç¯å¢ƒå˜é‡ï¼‰
    """
    try:
        # find_dotenv()ï¼šè‡ªåŠ¨æŸ¥æ‰¾å½“å‰ç›®å½•/ä¸Šçº§ç›®å½•çš„.envæ–‡ä»¶ï¼Œè¿”å›æ–‡ä»¶è·¯å¾„
        env_file_path = find_dotenv()
        if env_file_path:
            # åŠ è½½.envæ–‡ä»¶ï¼Œoverride=Falseï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¦†ç›–å·²æœ‰çš„ç³»ç»Ÿç¯å¢ƒå˜é‡
            load_dotenv(dotenv_path=env_file_path, override=False)
            st.success(f"âœ… æˆåŠŸåŠ è½½.envæ–‡ä»¶ï¼š{env_file_path}")
        else:
            st.info("â„¹ï¸ æœªæ‰¾åˆ°.envæ–‡ä»¶ï¼Œå°†ä¼˜å…ˆè¯»å–ç³»ç»Ÿç¯å¢ƒå˜é‡æˆ–æ‰‹åŠ¨è¾“å…¥API Key")
    except Exception as e:
        st.warning(f"âš ï¸ .envæ–‡ä»¶åŠ è½½å¤±è´¥ï¼š{str(e)}ï¼ˆä¸å½±å“åç»­æ“ä½œï¼‰")


# ====================== 2. å®šä¹‰æ¨¡å‹ä¾›åº”å•†é…ç½® ======================
MODEL_PROVIDERS = {
    "OpenAI": {
        "provider": "openai",
        "default_model": "gpt-3.5-turbo",
        "api_key_env": "OPENAI_API_KEY"
    },
    "Grok (XAI)": {
        "provider": "xai",
        "default_model": "grok-1",
        "api_key_env": "XAI_API_KEY"
    },
    "DeepSeek": {
        "provider": "deepseek",
        "default_model": "deepseek-chat",
        "api_key_env": "DEEPSEEK_API_KEY"
    },
    "Anthropic (Claude)": {
        "provider": "anthropic",
        "default_model": "claude-3-haiku-20240307",
        "api_key_env": "ANTHROPIC_API_KEY"
    }
}

# ====================== 3. Streamlitäº¤äº’ç•Œé¢ ======================
def get_model_credentials():
    """
    æ¸²æŸ“æ¨¡å‹ä¾›åº”å•†é€‰æ‹©å’ŒAPI Keyè¾“å…¥ç•Œé¢ï¼Œè¿”å›é€‰ä¸­çš„ä¾›åº”å•†é…ç½®å’ŒAPI Key
    ä¼˜å…ˆçº§ï¼šç³»ç»Ÿç¯å¢ƒå˜é‡ > .envæ–‡ä»¶å˜é‡ > æ‰‹åŠ¨è¾“å…¥
    """
    # å…ˆåŠ è½½.envæ–‡ä»¶ï¼ˆå…¨å±€æ‰§è¡Œï¼‰
    load_env_file()

    st.subheader("ğŸ”‘ å¤§æ¨¡å‹é…ç½®")
    
    # æ­¥éª¤1ï¼šä¸‹æ‹‰é€‰æ‹©æ¨¡å‹ä¾›åº”å•†
    selected_provider_name = st.selectbox(
        label="é€‰æ‹©æ¨¡å‹ä¾›åº”å•†",
        options=list(MODEL_PROVIDERS.keys()),
        index=0,
        help="æ”¯æŒOpenAIã€Grokã€DeepSeekã€Anthropicç­‰ä¾›åº”å•†"
    )
    
    # æ­¥éª¤2ï¼šè·å–è¯¥ä¾›åº”å•†çš„é…ç½®
    provider_config = MODEL_PROVIDERS[selected_provider_name]
    
    # æ­¥éª¤3ï¼šè¯»å–ç¯å¢ƒå˜é‡ï¼ˆå·²åŒ…å«.envåŠ è½½çš„å˜é‡ï¼‰ï¼Œæ— åˆ™æ˜¾ç¤ºè¾“å…¥æ¡†
    api_key = os.getenv(provider_config["api_key_env"])
    if not api_key:
        api_key = st.text_input(
            label=f"{selected_provider_name} API Key",
            type="password",
            help=f"è¯·è¾“å…¥{selected_provider_name}çš„API Keyï¼ˆå¯æå‰åœ¨.envæ–‡ä»¶ä¸­é…ç½®{provider_config['api_key_env']}ï¼‰"
        )
        # è¾“å…¥åè®¾ç½®ç¯å¢ƒå˜é‡
        if api_key:
            os.environ[provider_config["api_key_env"]] = api_key
    
    # æ­¥éª¤4ï¼šè‡ªå®šä¹‰æ¨¡å‹å
    model_name = st.text_input(
        label="æ¨¡å‹åç§°",
        value=provider_config["default_model"],
        help=f"{selected_provider_name}é»˜è®¤æ¨¡å‹ï¼š{provider_config['default_model']}"
    )
    
    # éªŒè¯API Key
    if not api_key:
        st.warning(f"è¯·è¾“å…¥{selected_provider_name}çš„API Keyï¼")
        return None, None, None
    
    return provider_config["provider"], model_name, api_key

# ====================== 4. åˆå§‹åŒ–å¤§æ¨¡å‹å‡½æ•° ======================
def init_llm_model(temperature=0.3):
    """åˆå§‹åŒ–å¤§æ¨¡å‹å®ä¾‹ï¼Œè¿”å›llmå¯¹è±¡"""
    model_provider, model_name, api_key = get_model_credentials()
    
    if not all([model_provider, model_name, api_key]):
        return None
    
    try:
        llm = init_chat_model(
            model_provider=model_provider,
            model=model_name,
            temperature=temperature,
            api_key=api_key
        )
        st.success(f"âœ… {model_provider} æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")
        return llm
    
    except LangChainException as e:
        st.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")
        return None
    except Exception as e:
        st.error(f"âŒ æœªçŸ¥é”™è¯¯ï¼š{str(e)}")
        return None

# ====================== 5. ä¸»æµç¨‹è°ƒç”¨ ======================
if __name__ == "__main__":
    st.title("å¤§æ¨¡å‹ä¾›åº”å•†é…ç½®ä¸åˆå§‹åŒ–")
    
    llm = init_llm_model(temperature=0.3)
    
    # æµ‹è¯•è°ƒç”¨
    if llm:
        if prompt := st.text_input("è¾“å…¥æµ‹è¯•prompt"):
            with st.spinner("æ¨¡å‹æ€è€ƒä¸­..."):
                response = llm.invoke(prompt)
                st.write("æ¨¡å‹å›å¤ï¼š", response.content)